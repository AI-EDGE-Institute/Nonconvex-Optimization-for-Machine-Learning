<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture Schedule</title>
    <style>
        table {
            width: 100%;
            border-collapse: collapse;
        }
        th, td {
            border: 1px solid #dddddd;
            text-align: left;
            padding: 8px;
        }
        th {
            background-color: #f2f2f2;
        }
    </style>
</head>
<body>

<h2>Nonconvex Optimization for Machine Learning</h2>

<table>
    <thead>
        <tr>
            <th scope="col">Module</th>
            <th scope="col">Lecture Topic</th>
            <th scope="col">Lecture Notes</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="text-align-justify">1. Course Info &amp; Introduction</td>
            <td class="text-align-justify">1. Course Info &amp; Introduction</td>
            <td class="text-align-justify"><a href="../Lecture Notes/LN1 Course Info/LN1.pdf">Lecture 1</a></td>
        </tr>
        <tr>
            <td class="text-align-justify" colspan="1" rowspan="6">2. First-Order Methods for Nonconvex Optimization</td>
            <td class="text-align-justify">2-1. Math Background Review</td>
            <td class="text-align-justify">Lecture 2-1</td>
        </tr>
        <tr>
            <td class="text-align-justify">2-2. Convexity</td>
            <td class="text-align-justify">Lecture 2-2</td>
        </tr>
        <tr>
            <td class="text-align-justify">2-3. Gradient Descent</td>
            <td class="text-align-justify">Lecture 2-3</td>
        </tr>
        <tr>
            <td class="text-align-justify">2-4. Stochastic Gradient Descent<br />
            (General Expectation Minimization,<br />
            Finite-Sum Minimization)</td>
            <td class="text-align-justify">Lecture 2-4</td>
        </tr>
        <tr>
            <td class="text-align-justify">2-5. Variance-Reduced Methods<br />
            (SAG, SVRG, SAGA, SPIDER, PAGE)</td>
            <td class="text-align-justify">Lecture 2-5</td>
        </tr>
        <tr>
            <td class="text-align-justify">2-6. Adaptive Methods<br />
            (AdaGrad, RMSProp, Adam)</td>
            <td class="text-align-justify">Lecture 2-6</td>
        </tr>
        <tr>
            <td class="text-align-justify" colspan="1" rowspan="2">3. Federated and Decentralized Learning</td>
            <td class="text-align-justify">3-1. Federated Learning<br />
            (Distributed Learning, FedAvg)</td>
            <td class="text-align-justify">Lecture 3-1</td>
        </tr>
        <tr>
            <td class="text-align-justify">3-2. Decentralized Learning<br />
            (Decentralized SGD, Gradient Tracking)</td>
            <td class="text-align-justify">Lecture 3-2</td>
        </tr>
        <tr>
            <td class="text-align-justify" colspan="1" rowspan="2">4. Zeroth-Order Methods for Nonconvex Optimization</td>
            <td class="text-align-justify">4-1. ZO Methods with Random Directions of Gradient Estimation</td>
            <td class="text-align-justify">Lecture 4-1</td>
        </tr>
        <tr>
            <td class="text-align-justify">4-2. Variance-Reduced Zeroth-Order Methods</td>
            <td class="text-align-justify">Lecture 4-2</td>
        </tr>
        <tr>
            <td class="text-align-justify" colspan="1" rowspan="2">5. First-Order Nonconvex Optimization with Special Geometric Structure</td>
            <td class="text-align-justify">5-1. The PL Condition and NTK</td>
            <td class="text-align-justify">Lecture 5-1</td>
        </tr>
        <tr>
            <td class="text-align-justify">5-2. NTK and Weak-Quasi-Convexity</td>
            <td class="text-align-justify">Lecture 5-2</td>
        </tr>
    </tbody>
</table>

</body>
</html>
